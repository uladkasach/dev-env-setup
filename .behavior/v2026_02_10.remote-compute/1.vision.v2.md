# vision: remote-compute v2 (all-cloud)

> pivot: what if we skip local hardware entirely and run all agent execution in the cloud?

## the outcome world

### before

- laptop fans spin constantly
- terminal input lags words behind
- claude code sessions compete with browser, slack for cpu/ram
- "let me close some stuff" becomes a daily ritual
- travel means degraded performance
- you own hardware that depreciates, breaks, needs maintenance

### after

- laptop runs a thin tui client — just keystrokes and display
- agents execute on cloud instances with unlimited scale
- spin up 64gb, 128gb, 256gb — whatever the task needs
- pay only for compute hours used
- same experience whether home, coffee shop, or airport
- no hardware to buy, maintain, or replace
- no home server active in a closet

### the "aha" moment

you're at an airport with a chromebook. you type a request into your thin tui. a 64gb ec2 instance spins up, clones your repo, runs 4 parallel claude agents, executes a full test suite, and returns results. total cost: $0.38. your chromebook never broke a sweat.

---

## user experience

### usecase 1: daily development

**goal**: run ai-assisted dev work without local compute

**timeline**:
1. open terminal on any device (laptop, tablet, chromebook)
2. `remote.dispatch "implement the user auth feature"`
3. thin tui forwards request to cloud orchestrator
4. orchestrator spins up instance, clones repo, runs agent
5. agent works autonomously — file edits, tests, commits
6. results stream back to tui as they complete
7. instance terminates when idle (or ttl expires)

**contract**:
```sh
# dispatch a task to the cloud
remote.dispatch "add rate limit to api endpoints"

# dispatch with specific compute requirements
remote.dispatch "run full integration suite" --instance m6i.4xlarge

# check status of active sessions
remote.status

# attach to a live session (watch agent work)
remote.attach session-abc123

# force terminate
remote.terminate session-abc123
```

### usecase 2: burst compute

**goal**: scale up for heavy tasks, scale down for light ones

**timeline**:
1. small task? → t3.medium (2 vcpu, 4gb) — $0.04/hr
2. big refactor? → m6i.4xlarge (16 vcpu, 64gb) — $0.77/hr
3. massive test suite? → m6i.8xlarge (32 vcpu, 128gb) — $1.54/hr
4. task completes → instance terminates → charges stop

**contract**:
```sh
# auto-select instance based on task complexity
remote.dispatch "fix typo in readme"  # → t3.small

# explicit instance for known-heavy tasks
remote.dispatch "run ml pipeline" --instance m6i.8xlarge --ttl 2h

# spot instances for cost savings (may be interrupted)
remote.dispatch "batch process logs" --spot --max-price 0.50
```

### usecase 3: persistent workspace

**goal**: maintain state between sessions for long projects

**timeline**:
1. create a persistent workspace (ebs volume)
2. workspace persists repos, caches, databases
3. attach workspace to any instance on demand
4. instance terminates, workspace persists
5. next session attaches same workspace — instant resume

**contract**:
```sh
# create persistent workspace
remote.workspace.create --name "project-alpha" --size 100gb

# dispatch with workspace attached
remote.dispatch "continue auth feature" --workspace project-alpha

# list workspaces
remote.workspace.list

# delete workspace (data gone forever)
remote.workspace.delete project-alpha
```

---

## mental model

### how you'd describe it to a friend

> "i type requests into a terminal. robots in the cloud do the actual work. i never think about cpu or ram — i just ask for what i want and it happens. like unlimited developers on call, each with their own beefy machine."

### analogies

- **lambda for dev work** — spin up compute, run task, terminate
- **github codespaces but headless** — no ide, just agents
- **your laptop is a tv remote; the cloud is the tv**

### terms

| user might say | we'd say |
|----------------|----------|
| "run this task" | `remote.dispatch` |
| "check on my task" | `remote.status` / `remote.attach` |
| "use a bigger machine" | `--instance m6i.4xlarge` |
| "keep my files around" | `--workspace` |
| "stop the machine" | `remote.terminate` / `--ttl` |

---

## architecture

```
┌─────────────────┐
│  local thin tui │  ← runs on any device (laptop, tablet, chromebook)
│  (keystrokes)   │
└────────┬────────┘
         │ https/wss
         ▼
┌─────────────────┐
│  orchestrator   │  ← api gateway + lambda (always on, ~$5/mo)
│  (dispatch)     │
└────────┬────────┘
         │ ec2 api
         ▼
┌─────────────────┐
│  compute pool   │  ← ec2 instances (on-demand, spot)
│  (agents run)   │
│                 │
│  - clone repo   │
│  - run claude   │
│  - execute cmds │
│  - stream logs  │
└────────┬────────┘
         │ ebs
         ▼
┌─────────────────┐
│  workspaces     │  ← persistent ebs volumes
│  (state)        │
└─────────────────┘
```

---

## evaluation

### how well does it solve the goals?

| goal | solved? |
|------|---------|
| laptop stops sluggish behavior | yes — compute offloaded entirely |
| input latency eliminated | partial — tui is local, but network round-trip for results |
| run many claude sessions | yes — unlimited cloud instances |
| works from anywhere | yes — same latency everywhere |
| no hardware maintenance | yes — zero to maintain |

### pros

| benefit | details |
|---------|---------|
| zero upfront cost | no $450 optiplex to buy |
| elastic scale | 4gb today, 256gb tomorrow |
| no maintenance | no hardware to break, clean, replace |
| location independent | same experience everywhere |
| pay per use | light month = low bill |

### cons / edgecases

| edgecase | mitigation |
|----------|------------|
| internet required | no offline work possible; accept this tradeoff |
| latency on results | ~50-100ms; acceptable for async agent work |
| cost can exceed hardware | ttl auto-shutdown, spot instances, usage alerts |
| cold start delay | ~30-60s to spin up; use persistent workspaces for fast resume |
| state loss on terminate | workspaces persist; ephemeral is default |
| vendor lock-in | abstract behind `remote.*` contract; swap providers later |

### cost scenarios

| usage pattern | monthly cost | vs optiplex breakeven |
|---------------|--------------|----------------------|
| 4 hrs/day, 20 days (t3.2xlarge) | ~$53 | 9 months |
| 8 hrs/day, 20 days (t3.2xlarge) | ~$106 | 4 months |
| 4 hrs/day, 20 days (m6i.4xlarge) | ~$123 | 4 months |
| always-on (t3.2xlarge) | ~$240 | 2 months |

**note**: cloud wins if usage is sporadic (<2 hrs/day avg) or you need burst capacity. hardware wins for steady daily use.

### pit of success

- `--ttl` default of 1h prevents runaway costs
- `remote.status` shows active sessions + spend
- daily cost alerts via sns
- spot instances for non-urgent work (70% savings)
- ephemeral by default — explicit `--workspace` for persistence

---

## open questions

1. **orchestrator complexity**: how much infra to build vs use prior art?
   - option a: custom orchestrator (lambda + step functions)
   - option b: aws batch / ecs tasks
   - option c: kubernetes (eks) with karpenter autoscale

2. **agent protocol**: how does thin tui communicate with remote agent?
   - option a: ssh tunnel (simple, battle-tested)
   - option b: websocket stream (lower latency, more complex)
   - option c: poll-based api (simplest, highest latency)

3. **secrets management**: how do agents access github, npm, etc?
   - option a: inject via env vars at launch
   - option b: aws secrets manager + iam roles
   - option c: vault integration

4. **repo sync**: how does agent get the code?
   - option a: git clone on every launch (slow but stateless)
   - option b: persistent workspace with git pull (fast resume)
   - option c: s3 sync of workspace snapshots

5. **when does cloud lose to hardware?**
   - if usage exceeds ~4 hrs/day consistently, hardware wins on cost
   - hybrid approach may be optimal: cloud for burst, hardware for baseline

---

## comparison: v1 (homelab) vs v2 (all-cloud)

| factor | v1 homelab | v2 all-cloud |
|--------|------------|--------------|
| upfront cost | $450 | $0 |
| monthly cost | ~$5 power | $50-250 (usage) |
| latency (home) | ~1ms | ~50-100ms |
| latency (travel) | n/a or cloud fallback | ~50-100ms |
| maintenance | you fix it | aws fixes it |
| scale | fixed (64gb) | elastic (4gb-256gb) |
| offline work | yes (on lan) | no |
| breakeven | ~6-12 months | n/a |

**v2 wins if**: you travel often, need burst compute, hate hardware, or usage is sporadic.

**v1 wins if**: you work from home daily, want lowest latency, prefer one-time cost.
